# ImageCaptioning
The caption generation problem involves training the machine to generate a textual description or caption for the image, given an image. This problem is an interesting one as it combines 2 prevalent areas in data science: Image Feature Extraction and Natural Language Processing. To solve this problem, an image-based model in required, which extracts the features out of the image, and the other is a language-based model, which translates the features given by the image-based model to a natural language sentence. It is expected that the generated caption describes in a sentence what is shown in the image, i.e., the objects present, their interactions, their properties, and the actions being performed. Since this model outputs a sentence, the most appropriate performance metric is the BLEU score. 

The data source being used is the Flickr8k dataset which contains 5 captions per image. The dataset contains a folder with 8091 images, each with 224 pixels x 224 pixels x 3 channels, with a unique image identifier as the file name. A separate text file is provided containing 5 captions for each image identifier. The Flickr8k dataset is selected to contain portion of each of the six different groups of the larger Flickr dataset. This dataset is not expected to be generate an effective model due to the size, but this size is chosen due to processing power limits. The dataset can be found on multiple GitHub repositories but there is a no redistribution clause provided by the University of Illinois. The dataset link is provided by the university to researchers after the following form is submitted: https://forms.illinois.edu/sec/1713398.

I was unable to upload the features.pkl file due to the size. Running the feature extraction takes approximately 2 hours without a GPU.
