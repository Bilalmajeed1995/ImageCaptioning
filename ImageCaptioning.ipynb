{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2811,
     "status": "ok",
     "timestamp": 1617147726737,
     "user": {
      "displayName": "Bilal Majeed",
      "photoUrl": "",
      "userId": "08823205380147123075"
     },
     "user_tz": 240
    },
    "id": "NI4eYAQG0NAd"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2811,
     "status": "ok",
     "timestamp": 1617147726731,
     "user": {
      "displayName": "Bilal Majeed",
      "photoUrl": "",
      "userId": "08823205380147123075"
     },
     "user_tz": 240
    },
    "id": "M-bxf0s20NAm"
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg19 import preprocess_input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9250052,
     "status": "ok",
     "timestamp": 1616911153988,
     "user": {
      "displayName": "Bilal Majeed",
      "photoUrl": "",
      "userId": "08823205380147123075"
     },
     "user_tz": 240
    },
    "id": "mqQPA35j0NAn",
    "outputId": "00464759-c0ae-4630-cfe6-975d429b9c7d"
   },
   "outputs": [],
   "source": [
    "def getFeatures(directory):\n",
    "    \n",
    "    # include_top = false -> model without dense layers, extract features only\n",
    "    # I will be training the image and text features together later to fine tune\n",
    "    model = VGG19(weights = 'imagenet')\n",
    "    model = Model(inputs = model.input, outputs = model.get_layer('fc2').output)\n",
    "    print(model.summary())\n",
    "    \n",
    "    features_dict = {}\n",
    "    \n",
    "    for file in listdir(directory):\n",
    "        # remove the extension of the file name\n",
    "        image_id = file.split('.')[0]\n",
    "        filename = directory + '/' + file\n",
    "        print(f\"Loading File Name: {file}, Total Files: {count}\")\n",
    "        # load image 224 x 224 with 3 channels\n",
    "        image = load_img(filename, target_size = (224, 224))\n",
    "        # convert image to 3D numpy array and reshape to 4D\n",
    "        image = img_to_array(image)\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # convert images from RGB to BGR, then each color channel is zero-centered \n",
    "        # with respect to the ImageNet dataset without scaling\n",
    "        image = preprocess_input(image)\n",
    "        feature = model.predict(image, verbose = 0)\n",
    "        features_dict[image_id] = feature\n",
    "        \n",
    "    return features_dict\n",
    "\n",
    "directory = 'Flickr8k_Dataset'\n",
    "features_dict = getFeatures(directory)\n",
    "print(f'Total Extracted Features: {len(features_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kExBUJQuRFv7"
   },
   "outputs": [],
   "source": [
    "from pickle import dump\n",
    "dump(features_dict, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4854,
     "status": "ok",
     "timestamp": 1617147757995,
     "user": {
      "displayName": "Bilal Majeed",
      "photoUrl": "",
      "userId": "08823205380147123075"
     },
     "user_tz": 240
    },
    "id": "BwVmDx8ORnY6"
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "features_dict = load(open('features.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 519,
     "status": "ok",
     "timestamp": 1617147761878,
     "user": {
      "displayName": "Bilal Majeed",
      "photoUrl": "",
      "userId": "08823205380147123075"
     },
     "user_tz": 240
    },
    "id": "z-FkQD4Z0NAp",
    "outputId": "558ea63e-3c6b-48ff-b97f-dfbd25763354"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Size: 6877\n",
      "Validation Set Size: 1214\n"
     ]
    }
   ],
   "source": [
    "from random import shuffle\n",
    "\n",
    "images = list(features_dict.keys())\n",
    "index_shuf = list(range(len(images)))\n",
    "shuffle(index_shuf)\n",
    "\n",
    "shuffled_images = []\n",
    "for i in range(len(images)):\n",
    "    shuffled_images += [images[index_shuf[i]]]\n",
    "\n",
    "train_split = int(round(len(images) * 0.85, 0))\n",
    "\n",
    "train_images = shuffled_images[ : train_split]\n",
    "print(f\"Training Set Size: {len(train_images)}\")\n",
    "val_images = shuffled_images[train_split : ]\n",
    "print(f\"Validation Set Size: {len(val_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 323,
     "status": "ok",
     "timestamp": 1617147768316,
     "user": {
      "displayName": "Bilal Majeed",
      "photoUrl": "",
      "userId": "08823205380147123075"
     },
     "user_tz": 240
    },
    "id": "3aVgUsDD0NAq"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def load_captions (file):\n",
    "    \n",
    "    file = open(filename, 'r')\n",
    "    captions = file.read()\n",
    "    file.close()\n",
    "    \n",
    "    captions_dict = {}\n",
    "    \n",
    "    for caption in captions.split('\\n'):\n",
    "        tokens = caption.split()\n",
    "        if len(caption) < 2:\n",
    "            continue\n",
    "        image_id, image_caption = tokens[0], tokens[1:]\n",
    "        image_id = image_id.split('.')[0]\n",
    "        image_caption = ' '.join(image_caption)\n",
    "        \n",
    "        if image_id not in captions_dict.keys():\n",
    "            captions_dict[image_id] = []\n",
    "            \n",
    "        captions_dict[image_id].append(image_caption)\n",
    "        \n",
    "    return captions_dict\n",
    "\n",
    "def clean_captions (captions_dict):\n",
    "    \n",
    "    punctuation = list(string.punctuation) + [''] + [\"\"]\n",
    "    \n",
    "    for image_id, captions in captions_dict.items():\n",
    "        for caption in range(len(captions)):\n",
    "            sentence = captions[caption]\n",
    "            sentence = sentence.split()\n",
    "            sentence = [word.lower() for word in sentence]\n",
    "            sentence = [word for word in sentence if word not in punctuation]\n",
    "            sentence = [word for word in sentence if len(word) > 1]\n",
    "            sentence = [word for word in sentence if word.isalpha()]\n",
    "            captions[caption] =  '<START> ' + ' '.join(sentence) + ' <END>'\n",
    "\n",
    "def load_dataset (captions_dict, features_dict, images):\n",
    "    \n",
    "    selected_captions = {}\n",
    "    selected_features = {}\n",
    "    \n",
    "    for image in images:\n",
    "        selected_features[image] = features_dict[image]\n",
    "        selected_captions[image] = captions_dict[image]\n",
    "        \n",
    "    return selected_captions, selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2258,
     "status": "ok",
     "timestamp": 1617147772401,
     "user": {
      "displayName": "Bilal Majeed",
      "photoUrl": "",
      "userId": "08823205380147123075"
     },
     "user_tz": 240
    },
    "id": "b-L1VI9p0NAq",
    "outputId": "bd2e312a-d02f-46ca-bc04-011b91ddb74c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images With Captions: 8092\n",
      "Training Images with Captions: 6877\n",
      "Training Images Features: 6877\n",
      "Validation Images with Captions: 1214\n",
      "Validation Images Features: 1214\n"
     ]
    }
   ],
   "source": [
    "filename = 'Flickr8k.token.txt'\n",
    "\n",
    "captions_dict = load_captions(filename)\n",
    "print(f\"Total Images With Captions: {len(captions_dict)}\")\n",
    "clean_captions(captions_dict)\n",
    "\n",
    "train_captions, train_features = load_dataset(captions_dict, features_dict, train_images)\n",
    "print(f\"Training Images with Captions: {len(train_captions)}\")\n",
    "print(f\"Training Images Features: {len(train_features)}\")\n",
    "\n",
    "val_captions, val_features = load_dataset(captions_dict, features_dict, val_images)\n",
    "print(f\"Validation Images with Captions: {len(val_captions)}\")\n",
    "print(f\"Validation Images Features: {len(val_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1617147778637,
     "user": {
      "displayName": "Bilal Majeed",
      "photoUrl": "",
      "userId": "08823205380147123075"
     },
     "user_tz": 240
    },
    "id": "qhcb0ffX0NAr"
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 514,
     "status": "ok",
     "timestamp": 1617147780881,
     "user": {
      "displayName": "Bilal Majeed",
      "photoUrl": "",
      "userId": "08823205380147123075"
     },
     "user_tz": 240
    },
    "id": "pgAWY-UR0NAr"
   },
   "outputs": [],
   "source": [
    "def flatten_captions (captions_dict):\n",
    "    \n",
    "    captions = []\n",
    "    \n",
    "    for image_id in captions_dict.keys():\n",
    "        [captions.append(caption) for caption in captions_dict[image_id]]\n",
    "        \n",
    "    return captions\n",
    "\n",
    "def create_tokenizer (captions):\n",
    "    \n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(captions)\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "def max_length (captions):\n",
    "    \n",
    "    return max(len(caption.split()) for caption in captions)\n",
    "\n",
    "def generate_sequences (tokenizer, max_len, captions_dict, features_dict, vocab_size):\n",
    "    \n",
    "    x1, x2, y = [], [], []\n",
    "    \n",
    "    for image_id, captions in captions_dict.items():\n",
    "        for caption in captions:\n",
    "            sequence = tokenizer.texts_to_sequences([caption])[0]\n",
    "            for i in range(1, len(sequence)):\n",
    "                input_sequence, output_sequence = sequence[:i], sequence[i]\n",
    "                input_sequence = pad_sequences([input_sequence], maxlen = max_len)[0]\n",
    "                output_sequence = to_categorical([output_sequence], num_classes = vocab_size)[0]\n",
    "                x1.append(features_dict[image_id][0])\n",
    "                x2.append(input_sequence)\n",
    "                y.append(output_sequence)\n",
    "    \n",
    "    return array(x1), array(x2), array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyDa_cex0NAs",
    "outputId": "48b4c7ee-930a-4efe-be97-1b32fe5dfe0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Vocabulary Size: 7780\n",
      "Maximum Caption Length: 33\n"
     ]
    }
   ],
   "source": [
    "captions_list = flatten_captions(train_captions)\n",
    "tokenizer = create_tokenizer(captions_list)\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "print(f\"Training Vocabulary Size: {vocabulary_size}\")\n",
    "\n",
    "max_len = max_length(captions_list)\n",
    "print(f\"Maximum Caption Length: {max_len}\")\n",
    "\n",
    "X1_train, X2_train, y_train = generate_sequences(tokenizer, max_len, train_captions, train_features, vocabulary_size)\n",
    "X1_val, X2_val, y_val = generate_sequences(tokenizer, max_len, val_captions, val_features, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SPgXNy-s0NAt"
   },
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def generate_model (vocab_size, max_len):\n",
    "    \n",
    "    image_features = Input(shape = (4096,))\n",
    "    tune_dropout = Dropout(0.5)(image_features)\n",
    "    tune_layer = Dense(512, activation = 'relu')(tune_dropout)\n",
    "\n",
    "    captions = Input(shape = (max_len,))\n",
    "    seq_enc_embed = Embedding(vocab_size, 512, mask_zero = True)(captions)\n",
    "    seq_enc_dropout = Dropout(0.5)(seq_enc_embed)\n",
    "    seq_enc_layer = LSTM(512)(seq_enc_dropout)\n",
    "\n",
    "    dec_combine = add([tune_layer, seq_enc_layer])\n",
    "    dec_layer = Dense(512, activation = 'relu')(dec_combine)\n",
    "    \n",
    "    output_caption = Dense(vocab_size, activation = 'softmax')(dec_layer)\n",
    "    \n",
    "    model = Model(inputs = [image_features, captions], outputs = output_caption)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "    \n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "93kWPKJM0NAu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 33)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 33, 512)      3983360     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 33, 512)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 512)          2097664     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 512)          2099200     dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 512)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 512)          262656      add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7780)         3991140     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,434,020\n",
      "Trainable params: 12,434,020\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "  149/10950 [..............................] - ETA: 48:11 - loss: 6.7271 - accuracy: 0.0949"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a250e480a51a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# define the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2_train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2_val\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = generate_model(vocabulary_size, max_len)\n",
    "history = model.fit([X1_train, X2_train], y_train, epochs = 5, verbose=1, validation_data=([X1_val, X2_val], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eJLfxm6c0NAv"
   },
   "outputs": [],
   "source": [
    "print(history.history['loss'])\n",
    "print(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2sKeap5a0NAv"
   },
   "outputs": [],
   "source": [
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bjN_kmqm0NAw"
   },
   "outputs": [],
   "source": [
    "# map an integer to a word\n",
    "def word_for_id (integer, tokenizer):\n",
    "    \n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "        \n",
    "    return '<UNK>'\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_caption (model, tokenizer, image, max_len):\n",
    "    \n",
    "    input_text = '<START>'\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        sequence = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen = max_len)\n",
    "        predicted = model.predict([image, sequence], verbose=0)\n",
    "        predicted = argmax(predicted)\n",
    "        word = word_for_id(predicted, tokenizer)\n",
    "        \n",
    "        if word == 'end':\n",
    "            input_text += ' ' + word\n",
    "            break\n",
    "        else:\n",
    "            input_text += ' ' + word\n",
    "            \n",
    "    return input_text\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model (model, tokenizer, max_len, captions_dict, features_dict):\n",
    "    \n",
    "    actual, predicted = [], []\n",
    "    \n",
    "    for image_id, captions in captions_dict.items():\n",
    "        yhat = generate_caption(model, tokenizer, features_dict[image_id], max_len)\n",
    "        actual.append([caption.split() for caption in captions])\n",
    "        predicted.append(yhat.split())\n",
    "        \n",
    "    print(f\"BLEU-1: {corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0))}\")\n",
    "    print(f\"BLEU-2: {corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0))}\")\n",
    "    print(f\"BLEU-3: {corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0))}\")\n",
    "    print(f\"BLEU-4: {corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25))}\")\n",
    "    \n",
    "    return actual, predicted\n",
    "\n",
    "actual_captions, predicted_captions = evaluate_model(model, tokenizer, max_len, val_captions, val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Image Captioning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
